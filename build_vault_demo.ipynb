{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q python-dotenv langchain langchain_core langchain-community langchain-chroma ipywidgets umap-learn pandas chromadb openai langchain-openai yt-dlp assemblyai matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import chromadb\n",
    "import openai\n",
    "import langchain_openai\n",
    "import yt_dlp\n",
    "import assemblyai\n",
    "import dotenv\n",
    "import pandas\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, Markdown, clear_output\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Configure Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure visualization\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline configuration\n",
    "config_path = os.path.join(os.getcwd(), 'pipeline_config.json')\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print(\"Loaded pipeline configuration\")\n",
    "else:\n",
    "    print(\"Warning: pipeline_config.json not found, using defaults\")\n",
    "    config = {}\n",
    "\n",
    "print(config.get('messages', {}).get('import_complete', 'Imports complete'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # loads .env if present\n",
    "\n",
    "required = ('YOUTUBE_API_KEY', 'ASSEMBLYAI_API_KEY', 'OPENAI_API_KEY')\n",
    "missing  = [var for var in required if not os.getenv(var)]\n",
    "\n",
    "if missing:\n",
    "    print(f\"Missing env vars: {', '.join(missing)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create local data storage (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), config.get('data_directory', '.'))\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "json_files = {\n",
    "    k: os.path.join(DATA_DIR, fname)\n",
    "    for k, fname in config.get('data_files', {}).items()\n",
    "}\n",
    "\n",
    "for path in json_files.values():\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump([], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing ChromaDB...\")\n",
    "\n",
    "# Create ChromaDB directory\n",
    "CHROMA_DIR = os.path.join(DATA_DIR, 'chromadb')\n",
    "os.makedirs(CHROMA_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize ChromaDB client with persistent storage\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DIR)\n",
    "\n",
    "# Create embedding function using OpenAI\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv('OPENAI_API_KEY'),\n",
    "    model_name=config.get('semantic_search', {}).get('embedding_model', os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small'))\n",
    ")\n",
    "\n",
    "# Create or get collections for segments and insights\n",
    "try:\n",
    "    segments_collection = chroma_client.get_or_create_collection(\n",
    "        name=\"segments\",\n",
    "        embedding_function=openai_ef,\n",
    "        metadata={\"description\": \"Episode transcript segments\"}\n",
    "    )\n",
    "    \n",
    "    insights_collection = chroma_client.get_or_create_collection(\n",
    "        name=\"insights\",\n",
    "        embedding_function=openai_ef,\n",
    "        metadata={\"description\": \"Extracted episode insights\"}\n",
    "    )\n",
    "    \n",
    "    print(f\"ChromaDB initialized successfully!\")\n",
    "    print(f\"  - Segments collection: {segments_collection.count()} items\")\n",
    "    print(f\"  - Insights collection: {insights_collection.count()} items\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error initializing ChromaDB: {e}\")\n",
    "    segments_collection = None\n",
    "    insights_collection = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Audio Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Download audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yt_dlp\n",
    "\n",
    "# Configuration\n",
    "YOUTUBE_URL = os.getenv('YOUTUBE_URL')\n",
    "\n",
    "# Extract video ID\n",
    "video_id = YOUTUBE_URL.split('v=')[-1].split('&')[0]\n",
    "audio_dir = os.path.join(os.getcwd(), config.get('audio_directory', 'audio_storage'))\n",
    "os.makedirs(audio_dir, exist_ok=True)\n",
    "\n",
    "# Check if episode exists\n",
    "with open(json_files['episodes'], 'r') as f:\n",
    "    episodes_data = json.load(f)\n",
    "existing = [ep for ep in episodes_data if ep['youtube_video_id'] == video_id]\n",
    "\n",
    "# Always download, regardless of whether episode exists\n",
    "print(f\"{config.get('messages', {}).get('downloading_audio', 'Downloading audio from:')} {YOUTUBE_URL}\")\n",
    "\n",
    "# Download with yt-dlp\n",
    "output_path = os.path.join(audio_dir, f'{video_id}.mp3')\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'mp3',\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "    'outtmpl': output_path.replace('.mp3', '.%(ext)s'),\n",
    "    'quiet': True,\n",
    "    'no_warnings': True,\n",
    "}\n",
    "\n",
    "try:\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(YOUTUBE_URL, download=True)\n",
    "        \n",
    "    # Create episode record\n",
    "    episode_id = str(uuid.uuid4())\n",
    "    episode_data = {\n",
    "        'id': episode_id,\n",
    "        'title': info.get('title', 'Unknown Title'),\n",
    "        'youtube_video_id': video_id,\n",
    "        'youtube_url': YOUTUBE_URL,\n",
    "        'duration': info.get('duration', 0),\n",
    "        'published_at': datetime.now().isoformat(),\n",
    "        'status': 'downloaded',\n",
    "        'audio_file_path': output_path,\n",
    "        'summary': '',\n",
    "        'is_processed': False\n",
    "    }\n",
    "    \n",
    "    # Save to JSON\n",
    "    episodes_data.append(episode_data)\n",
    "    with open(json_files['episodes'], 'w') as f:\n",
    "        json.dump(episodes_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Downloaded: {episode_data['title']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Download error: {e}\")\n",
    "    episode_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Display episode info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if episode_id:                      \n",
    "    print(\n",
    "        f\"Episode Info:\\n\"\n",
    "        f\"  Title: {episode_data['title']}\\n\"\n",
    "        f\"  YouTube ID: {video_id}\\n\"\n",
    "        f\"  Duration: {episode_data['duration']} s\\n\"\n",
    "        f\"  Status: {episode_data['status']}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Failed to download or load episode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 2: Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, uuid, assemblyai as aai\n",
    "\n",
    "if not episode_id:                               # ensure download step ran\n",
    "    raise RuntimeError(\"episode_id missing\")\n",
    "\n",
    "with open(json_files['segments']) as f:          # load existing segments\n",
    "    all_segments = json.load(f)\n",
    "\n",
    "segments = [s for s in all_segments if s['episode_id'] == episode_id]\n",
    "\n",
    "if not segments:                                 # need to transcribe\n",
    "    aai.settings.api_key = os.getenv('ASSEMBLYAI_API_KEY')\n",
    "    tr = aai.Transcriber().transcribe(\n",
    "        episode_data['audio_file_path'],\n",
    "        config=aai.TranscriptionConfig(speaker_labels=True, speakers_expected=2),\n",
    "    )\n",
    "    if tr.status == aai.TranscriptStatus.error:\n",
    "        raise RuntimeError(f\"AssemblyAI: {tr.error}\")\n",
    "\n",
    "    segments = [\n",
    "        {\n",
    "            'id': str(uuid.uuid4()),\n",
    "            'episode_id': episode_id,\n",
    "            'start_time': u.start / 1000,\n",
    "            'end_time':  u.end   / 1000,\n",
    "            'raw_text':  u.text,\n",
    "            'display_text': u.text,\n",
    "            'speaker': f\"Speaker {u.speaker}\",\n",
    "            'confidence': getattr(u, 'confidence', 0.9),\n",
    "            'duration': (u.end - u.start) / 1000,\n",
    "        }\n",
    "        for u in tr.utterances\n",
    "    ]\n",
    "    all_segments.extend(segments)\n",
    "    with open(json_files['segments'], 'w') as f:\n",
    "        json.dump(all_segments, f, indent=2)\n",
    "\n",
    "    print(\"Transcription completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Extract insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, uuid, traceback\n",
    "from openai import OpenAI\n",
    "import assemblyai as aai   # if still needed elsewhere\n",
    "\n",
    "if not episode_id or not segments:\n",
    "    raise RuntimeError(\"Missing episode_id or segmentsâ€”run previous steps first.\")\n",
    "\n",
    "SEGMENT_COUNT = len(segments)\n",
    "INSIGHT_BATCH = config.get('processing', {}).get('transcript_batch_size', 10)\n",
    "MAX_BATCHES   = config.get('processing', {}).get('max_transcript_batches', 10)\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "with open(json_files['insights']) as f:\n",
    "    all_insights = json.load(f)\n",
    "\n",
    "new_insights = []\n",
    "\n",
    "for i in range(0, min(SEGMENT_COUNT, INSIGHT_BATCH * MAX_BATCHES), INSIGHT_BATCH):\n",
    "    batch = segments[i : i + INSIGHT_BATCH]\n",
    "    transcript = \"\\n\".join(f\"{s['speaker']}: {s['display_text']}\" for s in batch)[:3500]\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=os.getenv('LLM_MODEL'),\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": config.get('llm_prompts', {}).get('insight_extraction', '')},\n",
    "                {\"role\": \"user\",   \"content\": f\"Extract insights from this podcast segment:\\n\\n{transcript}\"}\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        batch_data = json.loads(resp.choices[0].message.content).get('insights', [])\n",
    "        for ins in batch_data:\n",
    "            if ins.get('content'):\n",
    "                new_insights.append({\n",
    "                    'id': str(uuid.uuid4()),\n",
    "                    'episode_id': episode_id,\n",
    "                    'category': ins.get('category', 'Business Ideas'),\n",
    "                    'content': ins['content'],\n",
    "                    'confidence_score': float(ins.get('confidence', 0.8)),\n",
    "                    'segment_start': batch[0]['start_time'],\n",
    "                    'segment_end':   batch[-1]['end_time'],\n",
    "                })\n",
    "    except Exception:          # JSON errors, API errors, etc.\n",
    "        traceback.print_exc()   # optional: comment out to silence\n",
    "\n",
    "# save if any new insights\n",
    "if new_insights:\n",
    "    all_insights.extend(new_insights)\n",
    "    with open(json_files['insights'], 'w') as f:\n",
    "        json.dump(all_insights, f, indent=2)\n",
    "\n",
    "print(f\"Segments: {SEGMENT_COUNT}\")\n",
    "print(f\"Total insights extracted: {len(new_insights)}\")\n",
    "print(f\"Success: saved {len(new_insights)} insights to JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Display insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'insights' in locals() and insights:\n",
    "    display_insights = insights\n",
    "    \n",
    "    display(Markdown(\"### Extracted Insights\"))\n",
    "    \n",
    "    # Group by category\n",
    "    df_insights = pd.DataFrame(display_insights)\n",
    "    categories = df_insights['category'].value_counts()\n",
    "    \n",
    "    print(f\"\\nInsights by category:\")\n",
    "    for category, count in categories.items():\n",
    "        print(f\"   {category}: {count}\")\n",
    "    \n",
    "    # Display insights by category\n",
    "    for category, count in categories.items():\n",
    "        display(HTML(f\"<h4>{category} ({count})</h4>\"))\n",
    "        \n",
    "        cat_insights = df_insights[df_insights['category'] == category].head(3)\n",
    "        for _, insight in cat_insights.iterrows():\n",
    "            display(HTML(f\"\"\"\n",
    "            <div style='background: #f5f5f5; padding: 10px; margin: 5px 0; \n",
    "                        border-left: 3px solid #2196F3; border-radius: 5px;'>\n",
    "                <p style='margin: 0;'>{insight['content']}</p>\n",
    "                <small style='color: #666;'>Confidence: {insight['confidence_score']:.2f}</small>\n",
    "            </div>\n",
    "            \"\"\"))\n",
    "    \n",
    "    # Visualize distribution\n",
    "    if not categories.empty:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        categories.plot(kind='bar')\n",
    "        plt.title('Insight Distribution by Category')\n",
    "        plt.xlabel('Category')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"\\nNo insights to display. Check the debug output above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 3: Generate Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract products mentioned\n",
    "print(config.get('messages', {}).get('extracting_products', 'Extracting products from episode content...'))\n",
    "\n",
    "# Check if we have data to work with\n",
    "has_insights = 'insights' in locals() and insights\n",
    "has_segments = 'segments' in locals() and segments\n",
    "\n",
    "if not has_segments:\n",
    "    print(\"Error: No segments available. Please run transcription first.\")\n",
    "else:\n",
    "    # Read existing products\n",
    "    with open(json_files['products'], 'r') as f:\n",
    "        products_data = json.load(f)\n",
    "    products_found = []\n",
    "    \n",
    "    # Get known products from config\n",
    "    known_products = config.get('known_products', [])\n",
    "    \n",
    "    print(f\"Searching for {len(known_products)} known products...\")\n",
    "    \n",
    "    # Extract from insights if available\n",
    "    if has_insights:\n",
    "        print(\"\\nChecking insights for product mentions...\")\n",
    "        for insight in insights:\n",
    "            content = insight['content'].lower()\n",
    "            \n",
    "            for product in known_products:\n",
    "                if product.lower() in content:\n",
    "                    products_found.append({\n",
    "                        'name': product,\n",
    "                        'episode_id': episode_id,\n",
    "                        'mentioned_in': 'insight',\n",
    "                        'context': insight['content'][:200]\n",
    "                    })\n",
    "    \n",
    "    # Extract from transcript segments\n",
    "    demo_limit = config.get('processing', {}).get('demo_segment_limit', 50)\n",
    "    print(f\"\\nChecking {min(len(segments), demo_limit)} transcript segments...\")\n",
    "    for i, segment in enumerate(segments[:demo_limit]):\n",
    "        content = segment['display_text'].lower()\n",
    "        \n",
    "        for product in known_products:\n",
    "            if product.lower() in content:\n",
    "                products_found.append({\n",
    "                    'name': product,\n",
    "                    'episode_id': episode_id,\n",
    "                    'mentioned_in': 'transcript',\n",
    "                    'context': segment['display_text'][:200]\n",
    "                })\n",
    "    \n",
    "    print(f\"\\nFound {len(products_found)} total product mentions\")\n",
    "    \n",
    "    # Deduplicate and count mentions\n",
    "    product_counts = {}\n",
    "    for p in products_found:\n",
    "        name = p['name']\n",
    "        if name not in product_counts:\n",
    "            product_counts[name] = {\n",
    "                'count': 0,\n",
    "                'episode_ids': [],\n",
    "                'contexts': []\n",
    "            }\n",
    "        product_counts[name]['count'] += 1\n",
    "        if episode_id not in product_counts[name]['episode_ids']:\n",
    "            product_counts[name]['episode_ids'].append(episode_id)\n",
    "        product_counts[name]['contexts'].append(p['context'])\n",
    "    \n",
    "    print(f\"{len(product_counts)} unique products found\")\n",
    "    \n",
    "    # Update products data\n",
    "    new_products = []\n",
    "    updated_count = 0\n",
    "    \n",
    "    for name, data in product_counts.items():\n",
    "        # Check if product already exists\n",
    "        existing = next((p for p in products_data if p['name'] == name), None)\n",
    "        \n",
    "        if not existing:\n",
    "            # New product\n",
    "            new_products.append({\n",
    "                'id': str(uuid.uuid4()),\n",
    "                'name': name,\n",
    "                'episode_ids': data['episode_ids'],\n",
    "                'mention_count': data['count']\n",
    "            })\n",
    "        else:\n",
    "            # Update existing product\n",
    "            if episode_id not in existing['episode_ids']:\n",
    "                existing['episode_ids'].append(episode_id)\n",
    "            existing['mention_count'] += data['count']\n",
    "            updated_count += 1\n",
    "    \n",
    "    # Add new products\n",
    "    if new_products:\n",
    "        products_data.extend(new_products)\n",
    "        print(f\"Success: Added {len(new_products)} new products to database\")\n",
    "    \n",
    "    if updated_count:\n",
    "        print(f\"Success: Updated {updated_count} existing products\")\n",
    "    \n",
    "    # Save to JSON\n",
    "    with open(json_files['products'], 'w') as f:\n",
    "        json.dump(products_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nSuccess: Products extraction complete!\")\n",
    "    print(f\"   - Total unique products: {len(product_counts)}\")\n",
    "    print(f\"   - Total mentions: {sum(p['count'] for p in product_counts.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Display products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if product_counts:\n",
    "    display(Markdown(\"### Products Mentioned\"))\n",
    "    \n",
    "    # Sort by mention count\n",
    "    sorted_products = sorted(product_counts.items(), key=lambda x: x[1]['count'], reverse=True)\n",
    "    \n",
    "    # Show top products with context\n",
    "    display(Markdown(\"#### Top Products by Mentions:\"))\n",
    "    for i, (name, data) in enumerate(sorted_products[:10]):\n",
    "        # Get a sample context\n",
    "        sample_context = data['contexts'][0] if data['contexts'] else \"No context available\"\n",
    "        \n",
    "        display(HTML(f\"\"\"\n",
    "        <div style='background: white; border: 1px solid #e0e0e0; border-radius: 8px; \n",
    "                    padding: 15px; margin: 10px 0; box-shadow: 0 2px 4px rgba(0,0,0,0.1);'>\n",
    "            <h5 style='margin: 0 0 10px 0; color: #333;'>\n",
    "                {i+1}. {name} \n",
    "                <span style='background: #2196F3; color: white; padding: 2px 8px; \n",
    "                            border-radius: 12px; font-size: 12px; margin-left: 10px;'>\n",
    "                    {data['count']} mentions\n",
    "                </span>\n",
    "            </h5>\n",
    "            <p style='color: #666; font-size: 14px; margin: 0; font-style: italic;'>\n",
    "                \"{sample_context}...\"\n",
    "            </p>\n",
    "        </div>\n",
    "        \"\"\"))\n",
    "    \n",
    "    # Product cloud visualization\n",
    "    if len(sorted_products) > 5:\n",
    "        display(Markdown(\"#### Product Mention Distribution:\"))\n",
    "        \n",
    "        # Create bar chart\n",
    "        top_10 = sorted_products[:10]\n",
    "        names = [p[0] for p in top_10]\n",
    "        counts = [p[1]['count'] for p in top_10]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(names, counts, color='#2196F3')\n",
    "        plt.title('Top 10 Products by Mention Count')\n",
    "        plt.xlabel('Product')\n",
    "        plt.ylabel('Number of Mentions')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{int(height)}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"\\nNo products found. This could be because:\")\n",
    "    print(\"   - The episode doesn't mention any known products\")\n",
    "    print(\"   - The transcript is too short\")\n",
    "    print(\"   - The product list needs to be expanded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Extract links, insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "print(config.get('messages', {}).get('extracting_links', 'Extracting links from episode content...'))\n",
    "\n",
    "if 'episode_id' not in locals():\n",
    "    print(\"Error: Please run the Audio Download step first\")\n",
    "else:\n",
    "    # Read existing links\n",
    "    with open(json_files['links'], 'r') as f:\n",
    "        links_data = json.load(f)\n",
    "    found_links = []\n",
    "    \n",
    "    # Enhanced URL pattern to catch more URL formats\n",
    "    url_patterns = [\n",
    "        re.compile(r'https?://(?:www\\.)?[^\\s<>\"{}|\\\\^`\\[\\]]+'),\n",
    "        re.compile(r'(?:www\\.)[^\\s<>\"{}|\\\\^`\\[\\]]+\\.[a-z]{2,}'),\n",
    "        re.compile(r'[a-zA-Z0-9-]+\\.[a-z]{2,}/[^\\s<>\"{}|\\\\^`\\[\\]]*')\n",
    "    ]\n",
    "    \n",
    "    print(\"Searching for links in available content...\")\n",
    "    \n",
    "    # Extract from insights\n",
    "    if 'insights' in locals() and insights:\n",
    "        print(f\"\\nChecking {len(insights)} insights for links...\")\n",
    "        for insight in insights:\n",
    "            for pattern in url_patterns:\n",
    "                urls = pattern.findall(insight['content'])\n",
    "                for url in urls:\n",
    "                    # Normalize URL\n",
    "                    if not url.startswith('http'):\n",
    "                        url = 'https://' + url\n",
    "                    found_links.append({\n",
    "                        'url': url,\n",
    "                        'source': 'insight',\n",
    "                        'context': insight['content'][:150]\n",
    "                    })\n",
    "    \n",
    "    # Extract from transcript\n",
    "    if 'segments' in locals() and segments:\n",
    "        print(f\"\\nChecking {min(len(segments), 30)} transcript segments for links...\")\n",
    "        for segment in segments[:30]:  # Check first 30 segments\n",
    "            for pattern in url_patterns:\n",
    "                urls = pattern.findall(segment['display_text'])\n",
    "                for url in urls:\n",
    "                    if not url.startswith('http'):\n",
    "                        url = 'https://' + url\n",
    "                    found_links.append({\n",
    "                        'url': url,\n",
    "                        'source': 'transcript',\n",
    "                        'context': segment['display_text'][:150]\n",
    "                    })\n",
    "    \n",
    "    # Get website mentions from config\n",
    "    print(\"\\nChecking for mentioned websites...\")\n",
    "    website_mentions = config.get('website_mentions', {})\n",
    "    \n",
    "    # Search in all available text\n",
    "    all_text = \"\"\n",
    "    if 'segments' in locals() and segments:\n",
    "        all_text += \" \".join([s['display_text'] for s in segments[:50]])\n",
    "    if 'insights' in locals() and insights:\n",
    "        all_text += \" \".join([i['content'] for i in insights])\n",
    "    \n",
    "    all_text_lower = all_text.lower()\n",
    "    \n",
    "    for mention, url in website_mentions.items():\n",
    "        if mention in all_text_lower:\n",
    "            found_links.append({\n",
    "                'url': url,\n",
    "                'source': 'inferred',\n",
    "                'context': f'{mention.title()} was mentioned in the episode'\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nFound {len(found_links)} total link mentions\")\n",
    "    \n",
    "    # Deduplicate links\n",
    "    unique_links = {}\n",
    "    for link in found_links:\n",
    "        url = link['url'].rstrip('/').lower()  # Normalize URL\n",
    "        if url not in unique_links:\n",
    "            unique_links[url] = link\n",
    "    \n",
    "    print(f\"{len(unique_links)} unique links after deduplication\")\n",
    "    \n",
    "    # Create link records\n",
    "    new_links = []\n",
    "    for url, link_data in unique_links.items():\n",
    "        # Check if link already exists for this episode\n",
    "        existing = next((l for l in links_data if l['episode_id'] == episode_id and l['url'] == link_data['url']), None)\n",
    "        \n",
    "        if not existing:\n",
    "            # Extract title from URL\n",
    "            url_parts = link_data['url'].replace('https://', '').replace('http://', '').split('/')\n",
    "            domain = url_parts[0].replace('www.', '')\n",
    "            title = domain.split('.')[0].title() if domain else 'Link'\n",
    "            \n",
    "            # Better title extraction\n",
    "            if 'github.com' in link_data['url']:\n",
    "                title = 'GitHub: ' + '/'.join(url_parts[1:3]) if len(url_parts) > 2 else 'GitHub'\n",
    "            elif 'youtube.com' in link_data['url'] or 'youtu.be' in link_data['url']:\n",
    "                title = 'YouTube Video'\n",
    "            elif link_data['source'] == 'inferred':\n",
    "                title = link_data['context']\n",
    "            \n",
    "            new_links.append({\n",
    "                'id': str(uuid.uuid4()),\n",
    "                'episode_id': episode_id,\n",
    "                'url': link_data['url'],\n",
    "                'title': title,\n",
    "                'description': f\"Found in {link_data['source']}: {link_data['context']}\",\n",
    "                'enriched': False\n",
    "            })\n",
    "    \n",
    "    # Add new links\n",
    "    if new_links:\n",
    "        links_data.extend(new_links)\n",
    "        with open(json_files['links'], 'w') as f:\n",
    "            json.dump(links_data, f, indent=2)\n",
    "        print(f\"\\nSuccess: Saved {len(new_links)} new links to JSON\")\n",
    "    else:\n",
    "        print(\"\\nNo new links found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Generate embeddings and store in ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if episode_id and segments_collection and insights_collection:\n",
    "    print(config.get('messages', {}).get('generating_embeddings', 'Generating vector embeddings and storing in ChromaDB...'))\n",
    "    \n",
    "    try:\n",
    "        # Load segments and insights\n",
    "        with open(json_files['segments'], 'r') as f:\n",
    "            segments_data = json.load(f)\n",
    "        with open(json_files['insights'], 'r') as f:\n",
    "            insights_data = json.load(f)\n",
    "        \n",
    "        # Filter for current episode\n",
    "        episode_segments = [s for s in segments_data if s['episode_id'] == episode_id]\n",
    "        episode_insights = [i for i in insights_data if i['episode_id'] == episode_id]\n",
    "        \n",
    "        # Check existing embeddings in ChromaDB\n",
    "        existing_segment_ids = []\n",
    "        existing_insight_ids = []\n",
    "        \n",
    "        try:\n",
    "            # Query for existing segments\n",
    "            segment_results = segments_collection.get(\n",
    "                where={\"episode_id\": episode_id}\n",
    "            )\n",
    "            existing_segment_ids = segment_results['ids'] if segment_results else []\n",
    "            \n",
    "            # Query for existing insights\n",
    "            insight_results = insights_collection.get(\n",
    "                where={\"episode_id\": episode_id}\n",
    "            )\n",
    "            existing_insight_ids = insight_results['ids'] if insight_results else []\n",
    "        except:\n",
    "            # Collections might be empty\n",
    "            pass\n",
    "        \n",
    "        # Filter out already embedded items\n",
    "        segments_to_embed = [s for s in episode_segments if s['id'] not in existing_segment_ids]\n",
    "        insights_to_embed = [i for i in episode_insights if i['id'] not in existing_insight_ids]\n",
    "        \n",
    "        total_needed = len(segments_to_embed) + len(insights_to_embed)\n",
    "        \n",
    "        if total_needed > 0:\n",
    "            print(f\"Need to generate embeddings for:\")\n",
    "            if segments_to_embed:\n",
    "                print(f\"   - {len(segments_to_embed)} new segments\")\n",
    "            if insights_to_embed:\n",
    "                print(f\"   - {len(insights_to_embed)} new insights\")\n",
    "            \n",
    "            # Add segments to ChromaDB\n",
    "            if segments_to_embed:\n",
    "                print(\"\\nAdding segments to ChromaDB...\")\n",
    "                \n",
    "                # Prepare data for ChromaDB\n",
    "                segment_ids = [s['id'] for s in segments_to_embed]\n",
    "                segment_texts = [s['display_text'] for s in segments_to_embed]\n",
    "                segment_metadatas = [{\n",
    "                    'episode_id': s['episode_id'],\n",
    "                    'start_time': s['start_time'],\n",
    "                    'end_time': s['end_time'],\n",
    "                    'speaker': s['speaker'],\n",
    "                    'duration': s['duration']\n",
    "                } for s in segments_to_embed]\n",
    "                \n",
    "                # Add to ChromaDB in batches\n",
    "                batch_size = config.get('semantic_search', {}).get('batch_size', 20)\n",
    "                \n",
    "                for i in range(0, len(segments_to_embed), batch_size):\n",
    "                    batch_end = min(i + batch_size, len(segments_to_embed))\n",
    "                    print(f\"   Processing batch {i//batch_size + 1}/{(len(segments_to_embed)-1)//batch_size + 1}...\")\n",
    "                    \n",
    "                    segments_collection.add(\n",
    "                        documents=segment_texts[i:batch_end],\n",
    "                        ids=segment_ids[i:batch_end],\n",
    "                        metadatas=segment_metadatas[i:batch_end]\n",
    "                    )\n",
    "                \n",
    "                print(f\"Success: Added {len(segments_to_embed)} segments to ChromaDB\")\n",
    "            \n",
    "            # Add insights to ChromaDB\n",
    "            if insights_to_embed:\n",
    "                print(\"\\nAdding insights to ChromaDB...\")\n",
    "                \n",
    "                # Prepare data for ChromaDB\n",
    "                insight_ids = [i['id'] for i in insights_to_embed]\n",
    "                insight_texts = [i['content'] for i in insights_to_embed]\n",
    "                insight_metadatas = [{\n",
    "                    'episode_id': i['episode_id'],\n",
    "                    'category': i['category'],\n",
    "                    'confidence_score': i['confidence_score'],\n",
    "                    'segment_start': i['segment_start'],\n",
    "                    'segment_end': i['segment_end']\n",
    "                } for i in insights_to_embed]\n",
    "                \n",
    "                # Add all insights at once (usually fewer than segments)\n",
    "                insights_collection.add(\n",
    "                    documents=insight_texts,\n",
    "                    ids=insight_ids,\n",
    "                    metadatas=insight_metadatas\n",
    "                )\n",
    "                \n",
    "                print(f\"Success: Added {len(insights_to_embed)} insights to ChromaDB\")\n",
    "            \n",
    "            # Display summary\n",
    "            display(HTML(f\"\"\"\n",
    "            <div style='background: #e8f5e9; padding: 15px; border-radius: 5px; margin-top: 10px;'>\n",
    "                <strong>ChromaDB Embeddings Update Complete!</strong><br>\n",
    "                <ul style='margin: 5px 0;'>\n",
    "                    <li>New segments added: {len(segments_to_embed)}</li>\n",
    "                    <li>New insights added: {len(insights_to_embed)}</li>\n",
    "                    <li>Total segments in DB: {segments_collection.count()}</li>\n",
    "                    <li>Total insights in DB: {insights_collection.count()}</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "            \"\"\"))\n",
    "            \n",
    "        else:\n",
    "            print(\"Success: All items already have embeddings in ChromaDB\")\n",
    "            print(f\"   Existing segments: {len(existing_segment_ids)}\")\n",
    "            print(f\"   Existing insights: {len(existing_insight_ids)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: ChromaDB operation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"Error: Missing required data (episode_id or ChromaDB collections not initialized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Final summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'episode_id' in locals() and episode_id:\n",
    "    # Mark as processed\n",
    "    with open(json_files['episodes'], 'r') as f:\n",
    "        episodes_data = json.load(f)\n",
    "    \n",
    "    for episode in episodes_data:\n",
    "        if episode['id'] == episode_id:\n",
    "            episode['is_processed'] = True\n",
    "            break\n",
    "    \n",
    "    with open(json_files['episodes'], 'w') as f:\n",
    "        json.dump(episodes_data, f, indent=2)\n",
    "    \n",
    "    # Gather statistics\n",
    "    stats = {\n",
    "        \"Episode\": episode_data['title'] if 'episode_data' in locals() else 'Unknown',\n",
    "        \"Duration\": f\"{episode_data['duration']} seconds\" if 'episode_data' in locals() else 'Unknown',\n",
    "        \"Segments\": len(segments) if 'segments' in locals() else 0,\n",
    "        \"Insights\": len(insights) if 'insights' in locals() else 0,\n",
    "        \"Status\": \"Processed\"\n",
    "    }\n",
    "    \n",
    "    # Add ChromaDB stats\n",
    "    if segments_collection and insights_collection:\n",
    "        stats[\"ChromaDB Segments\"] = segments_collection.count()\n",
    "        stats[\"ChromaDB Insights\"] = insights_collection.count()\n",
    "    \n",
    "    # Display summary\n",
    "    display(Markdown(f\"## {config.get('messages', {}).get('pipeline_complete', 'Pipeline Complete')}\"))\n",
    "    \n",
    "    stats_html = \"\"\"\n",
    "    <div style='display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); \n",
    "                gap: 15px; margin: 20px 0;'>\n",
    "    \"\"\"\n",
    "    \n",
    "    colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c', '#9b59b6', '#1abc9c', '#34495e']\n",
    "    \n",
    "    for i, (stat, value) in enumerate(stats.items()):\n",
    "        color = colors[i % len(colors)]\n",
    "        stats_html += f\"\"\"\n",
    "        <div style='background: {color}; color: white; padding: 20px; \n",
    "                    border-radius: 10px; text-align: center;'>\n",
    "            <div style='font-size: 24px; font-weight: bold;'>{value}</div>\n",
    "            <div style='font-size: 14px; margin-top: 5px;'>{stat}</div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    stats_html += \"</div>\"\n",
    "    display(HTML(stats_html))\n",
    "    \n",
    "    display(Markdown(f\"\"\"\n",
    "### Data Storage\n",
    "\n",
    "Your data has been saved to:\n",
    "\n",
    "#### JSON Files (in `{config.get('data_directory', './')}` directory):\n",
    "- `{config.get('data_files', {}).get('episodes', 'episodes.json')}` - Episode metadata\n",
    "- `{config.get('data_files', {}).get('segments', 'segments.json')}` - Transcription segments\n",
    "- `{config.get('data_files', {}).get('insights', 'insights.json')}` - Extracted insights\n",
    "- `{config.get('data_files', {}).get('products', 'products.json')}` - Product mentions\n",
    "- `{config.get('data_files', {}).get('links', 'links.json')}` - Extracted links\n",
    "\n",
    "#### ChromaDB Vector Database (in `{config.get('data_directory', './')}/chromadb` directory):\n",
    "- **Segments Collection**: {segments_collection.count() if segments_collection else 0} embedded transcript segments\n",
    "- **Insights Collection**: {insights_collection.count() if insights_collection else 0} embedded insights\n",
    "- Persistent storage with automatic embedding generation\n",
    "- Supports semantic search with metadata filtering\n",
    "\n",
    "You can now:\n",
    "- Analyze the JSON data using pandas or other tools\n",
    "- Perform semantic searches using ChromaDB\n",
    "- Add more episodes and they'll be automatically indexed\n",
    "- Query across multiple episodes using metadata filters\n",
    "    \"\"\"))\n",
    "else:\n",
    "    print(\"Error: No episode was processed. Please run the notebook cells in order from the beginning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ChromaDB Viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "# Load configuration settings from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the path to the ChromaDB created by build_vault_demo.ipynb\n",
    "# The build_vault_demo.ipynb creates the database at ./local_data/chromadb\n",
    "CHROMA_DB_PATH = os.path.join(os.getcwd(), 'local_data', 'chromadb')\n",
    "\n",
    "# Check if the path exists\n",
    "if not os.path.exists(CHROMA_DB_PATH):\n",
    "    print(f\"ChromaDB path not found at: {CHROMA_DB_PATH}\")\n",
    "    print(\"Please run build_vault_demo.ipynb first to create the database.\")\n",
    "else:\n",
    "    print(f\"ChromaDB loaded successfully\")\n",
    "\n",
    "# Set up chromadb client\n",
    "client = chromadb.PersistentClient(\n",
    "    path=CHROMA_DB_PATH,\n",
    "    settings=Settings(),\n",
    "    tenant=DEFAULT_TENANT,\n",
    "    database=DEFAULT_DATABASE,\n",
    ")\n",
    "\n",
    "# Function to list collections\n",
    "def list_collections():\n",
    "    collections = client.list_collections()\n",
    "    print(f\"Found {len(collections)} collections\")  # Debug print\n",
    "    return [collection.name for collection in collections]\n",
    "\n",
    "# Function to format embedding preview\n",
    "def format_embedding_preview(embedding, preview_length=10):\n",
    "    \"\"\"Format embedding vector for display\"\"\"\n",
    "    if embedding is None or len(embedding) == 0:\n",
    "        return \"N/A\"\n",
    "    \n",
    "    # Convert to numpy array if needed\n",
    "    if isinstance(embedding, list):\n",
    "        embedding = np.array(embedding)\n",
    "    \n",
    "    # Get first few values\n",
    "    preview = embedding[:preview_length]\n",
    "    preview_str = \", \".join([f\"{x:.4f}\" for x in preview])\n",
    "    \n",
    "    # Add statistics\n",
    "    stats = f\"[{preview_str}, ...] (dim: {len(embedding)}, mean: {np.mean(embedding):.4f}, std: {np.std(embedding):.4f})\"\n",
    "    return stats\n",
    "\n",
    "# Function to display collection details\n",
    "def show_details(collection_name):\n",
    "    try:\n",
    "        collection = client.get_collection(collection_name)\n",
    "        details_output.clear_output()\n",
    "        with details_output:\n",
    "            print(f\"Collection Name: {collection_name}\")\n",
    "            print(f\"Document Count: {collection.count()}\")\n",
    "            if collection.metadata:\n",
    "                for key, value in collection.metadata.items():\n",
    "                    print(f\"{key}: {value}\")\n",
    "            else:\n",
    "                print(\"No metadata available.\")\n",
    "            \n",
    "            # Display details of the documents in the collection\n",
    "            # Include embeddings in the query\n",
    "            raw_data = collection.get(limit=5, include=['metadatas', 'documents', 'embeddings'])\n",
    "            print(\"\\nSample Documents:\")  # Better formatting\n",
    "            \n",
    "            # Extract and display relevant data\n",
    "            ids = raw_data.get('ids', [])\n",
    "            metadatas = raw_data.get('metadatas', [])\n",
    "            documents = raw_data.get('documents', [])\n",
    "            embeddings = raw_data.get('embeddings', [])\n",
    "\n",
    "            if not ids or not metadatas:\n",
    "                print(\"No documents available or missing data.\")\n",
    "                return\n",
    "\n",
    "            # Create table with clickable IDs and embedding preview\n",
    "            table_html = \"<table style='border-collapse: collapse; width: 100%;'>\"\n",
    "            table_html += \"<tr style='background-color: #f2f2f2;'>\"\n",
    "            table_html += \"<th style='border: 1px solid #ddd; padding: 8px;'>ID</th>\"\n",
    "            table_html += \"<th style='border: 1px solid #ddd; padding: 8px;'>Metadata</th>\"\n",
    "            table_html += \"<th style='border: 1px solid #ddd; padding: 8px;'>Document Preview</th>\"\n",
    "            table_html += \"<th style='border: 1px solid #ddd; padding: 8px;'>Embedding Preview</th>\"\n",
    "            table_html += \"</tr>\"\n",
    "            \n",
    "            for i, (doc_id, metadata) in enumerate(zip(ids, metadatas)):\n",
    "                doc_preview = documents[i][:100] + \"...\" if documents and i < len(documents) else \"N/A\"\n",
    "                \n",
    "                # Fixed: Handle embeddings properly regardless of type\n",
    "                embedding_preview = \"N/A\"\n",
    "                if embeddings is not None:\n",
    "                    try:\n",
    "                        if isinstance(embeddings, (list, tuple)) and len(embeddings) > i:\n",
    "                            embedding_preview = format_embedding_preview(embeddings[i])\n",
    "                        elif isinstance(embeddings, np.ndarray) and embeddings.shape[0] > i:\n",
    "                            embedding_preview = format_embedding_preview(embeddings[i])\n",
    "                    except:\n",
    "                        embedding_preview = \"N/A\"\n",
    "                \n",
    "                table_html += f\"<tr>\"\n",
    "                table_html += f\"<td style='border: 1px solid #ddd; padding: 8px;'><a href='#' onclick='IPython.notebook.kernel.execute(\\\"show_document_details(\\\\\\\"{collection_name}\\\\\\\", \\\\\\\"{doc_id}\\\\\\\")\\\")'>{doc_id[:8]}...</a></td>\"\n",
    "                table_html += f\"<td style='border: 1px solid #ddd; padding: 8px;'>{metadata}</td>\"\n",
    "                table_html += f\"<td style='border: 1px solid #ddd; padding: 8px;'>{doc_preview}</td>\"\n",
    "                table_html += f\"<td style='border: 1px solid #ddd; padding: 8px; font-family: monospace; font-size: 11px;'>{embedding_preview}</td>\"\n",
    "                table_html += \"</tr>\"\n",
    "            table_html += \"</table>\"\n",
    "            \n",
    "            display(HTML(table_html))\n",
    "    except Exception as e:\n",
    "        details_output.clear_output()\n",
    "        with details_output:\n",
    "            print(f\"Error while fetching details for collection '{collection_name}': {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# Function to display individual document details\n",
    "def show_document_details(collection_name, doc_id):\n",
    "    try:\n",
    "        collection = client.get_collection(collection_name)\n",
    "        document_details_output.clear_output()\n",
    "        with document_details_output:\n",
    "            print(f\"Collection: {collection_name}, Document ID: {doc_id}\")\n",
    "            \n",
    "            # Retrieve document details including embeddings\n",
    "            raw_data = collection.get(ids=[doc_id], include=['metadatas', 'documents', 'embeddings'])\n",
    "            if not raw_data or not raw_data.get('metadatas'):\n",
    "                print(f\"No details available for document ID: {doc_id}\")\n",
    "                return\n",
    "\n",
    "            # Assuming getting first (and only one) document metadata and embeddings\n",
    "            metadata = raw_data['metadatas'][0]\n",
    "            document = raw_data['documents'][0] if raw_data.get('documents') else \"N/A\"\n",
    "            \n",
    "            # Handle embeddings safely\n",
    "            embedding = None\n",
    "            embeddings = raw_data.get('embeddings')\n",
    "            if embeddings is not None:\n",
    "                try:\n",
    "                    if isinstance(embeddings, (list, tuple)) and len(embeddings) > 0:\n",
    "                        embedding = embeddings[0]\n",
    "                    elif isinstance(embeddings, np.ndarray) and embeddings.shape[0] > 0:\n",
    "                        embedding = embeddings[0]\n",
    "                except:\n",
    "                    embedding = None\n",
    "            \n",
    "            print(\"\\nMetadata:\")\n",
    "            for key, value in metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "            \n",
    "            print(\"\\nDocument Content:\")\n",
    "            print(f\"  {document}\")\n",
    "            \n",
    "            if embedding is not None:\n",
    "                print(\"\\nEmbedding Information:\")\n",
    "                print(f\"  Dimension: {len(embedding)}\")\n",
    "                print(f\"  Mean: {np.mean(embedding):.6f}\")\n",
    "                print(f\"  Std Dev: {np.std(embedding):.6f}\")\n",
    "                print(f\"  Min: {np.min(embedding):.6f}\")\n",
    "                print(f\"  Max: {np.max(embedding):.6f}\")\n",
    "                print(f\"  First 20 values: {embedding[:20]}\")\n",
    "                \n",
    "                # Optional: Create a simple visualization of the embedding\n",
    "                try:\n",
    "                    import matplotlib.pyplot as plt\n",
    "                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "                    \n",
    "                    # Histogram of embedding values\n",
    "                    ax1.hist(embedding, bins=50, alpha=0.7, color='blue')\n",
    "                    ax1.set_xlabel('Embedding Value')\n",
    "                    ax1.set_ylabel('Frequency')\n",
    "                    ax1.set_title('Distribution of Embedding Values')\n",
    "                    \n",
    "                    # Plot first 100 dimensions\n",
    "                    ax2.plot(embedding[:100], alpha=0.7, color='green')\n",
    "                    ax2.set_xlabel('Dimension')\n",
    "                    ax2.set_ylabel('Value')\n",
    "                    ax2.set_title('First 100 Embedding Dimensions')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not create visualization: {e}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        document_details_output.clear_output()\n",
    "        with document_details_output:\n",
    "            print(f\"Error while fetching details for document ID '{doc_id}': {str(e)}\")\n",
    "\n",
    "\n",
    "# Function to delete collection\n",
    "def delete_collection(collection_name):\n",
    "    try:\n",
    "        client.delete_collection(collection_name)\n",
    "        refresh_collections()\n",
    "    except Exception as e:\n",
    "        error_output.clear_output()\n",
    "        with error_output:\n",
    "            print(f\"Error while deleting collection '{collection_name}': {str(e)}\")\n",
    "\n",
    "# Function to refresh the list of collections\n",
    "def refresh_collections():\n",
    "    try:\n",
    "        collection_names = list_collections()\n",
    "        print(f\"Refreshing with {len(collection_names)} collections: {collection_names}\")  # Debug print\n",
    "        \n",
    "        if not collection_names:\n",
    "            collection_buttons.children = [widgets.Label(value=\"No collections found in the database.\")]\n",
    "        else:\n",
    "            collection_buttons.children = [\n",
    "                widgets.HBox([\n",
    "                    widgets.Label(value=collection_name, layout=widgets.Layout(width='200px')),\n",
    "                    widgets.Button(description=\"Delete\", button_style='danger', layout=widgets.Layout(width='100px')),\n",
    "                    widgets.Button(description=\"Details\", button_style='info', layout=widgets.Layout(width='100px'))\n",
    "                ])\n",
    "                for collection_name in collection_names\n",
    "            ]\n",
    "            \n",
    "            for box in collection_buttons.children:\n",
    "                name_label, delete_button, details_button = box.children\n",
    "                \n",
    "                # Use default arguments in lambda to capture the current collection name\n",
    "                delete_button.on_click(lambda b, name=name_label.value: delete_collection(name))\n",
    "                details_button.on_click(lambda b, name=name_label.value: show_details(name))\n",
    "    except Exception as e:\n",
    "        print(f\"Error refreshing collections: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Set up widgets\n",
    "details_output = widgets.Output()\n",
    "document_details_output = widgets.Output()\n",
    "error_output = widgets.Output()\n",
    "collection_buttons = widgets.VBox()\n",
    "refresh_button = widgets.Button(description=\"Refresh Collections\", button_style='primary')\n",
    "refresh_button.on_click(lambda b: refresh_collections())\n",
    "\n",
    "# Display widgets in the Jupyter notebook\n",
    "display(refresh_button, collection_buttons, details_output, document_details_output, error_output)\n",
    "\n",
    "# Initialize the list of collections\n",
    "refresh_collections()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
