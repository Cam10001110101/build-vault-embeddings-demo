{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Podcast GraphRAG with Neo4j\n",
    "\n",
    "This notebook demonstrates a Graph-based Question Answering system using Neo4j and LangChain for YouTube podcast data. It creates a knowledge graph from podcast transcripts, insights, and product mentions to enable complex queries about the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-groq neo4j langchain langchain-community langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# API Keys\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Neo4j Connection\n",
    "NEO4J_URI = os.getenv('NEO4J_URI') \n",
    "NEO4J_USER = os.getenv('NEO4J_USER')\n",
    "NEO4J_PW = os.getenv('NEO4J_PW')\n",
    "\n",
    "# Local LLM (Ollama)\n",
    "LLM_BASE_URL = os.getenv('LLM_BASE_URL')\n",
    "LLM_MODEL = os.getenv('LLM_MODEL')\n",
    "LLM_TEMPERATURE = float(os.getenv('LLM_TEMPERATURE', '0.0'))\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'local_data')\n",
    "\n",
    "print(\"Environment loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_groq import ChatGroq\n",
    "from neo4j import GraphDatabase\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 episodes\n",
      "Loaded 90 segments\n",
      "Loaded 0 insights\n",
      "Loaded 24 products\n",
      "Loaded 3 links\n"
     ]
    }
   ],
   "source": [
    "# Load pipeline configuration\n",
    "config_path = os.path.join(os.getcwd(), 'pipeline_config.json')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Define data file paths\n",
    "json_files = {\n",
    "    'episodes': os.path.join(DATA_DIR, config['data_files']['episodes']),\n",
    "    'segments': os.path.join(DATA_DIR, config['data_files']['segments']),\n",
    "    'insights': os.path.join(DATA_DIR, config['data_files']['insights']),\n",
    "    'products': os.path.join(DATA_DIR, config['data_files']['products']),\n",
    "    'links': os.path.join(DATA_DIR, config['data_files']['links'])\n",
    "}\n",
    "\n",
    "# Load all data\n",
    "data = {}\n",
    "for key, path in json_files.items():\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'r') as f:\n",
    "            data[key] = json.load(f)\n",
    "            print(f\"Loaded {len(data[key])} {key}\")\n",
    "    else:\n",
    "        data[key] = []\n",
    "        print(f\"Warning: {key} file not found at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo4j connection successful!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Neo4j driver for data population\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PW))\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"RETURN 1\")\n",
    "        print(\"Neo4j connection successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Neo4j connection failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear Existing Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database cleared\n"
     ]
    }
   ],
   "source": [
    "# Clear existing data - uncomment to reset database\n",
    "with driver.session() as session:\n",
    "     session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "     print(\"Database cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: (e:Episode)\n",
      "Created: (s:Segment)\n",
      "Created: (sp:Speaker)\n",
      "Created: (i:Insight)\n",
      "Created: (p:Product)\n",
      "Created: (t:Topic)\n"
     ]
    }
   ],
   "source": [
    "# Create constraints and indexes for better performance\n",
    "with driver.session() as session:\n",
    "    # Create constraints\n",
    "    constraints = [\n",
    "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (e:Episode) REQUIRE e.id IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (s:Segment) REQUIRE s.id IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (sp:Speaker) REQUIRE sp.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (i:Insight) REQUIRE i.id IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (p:Product) REQUIRE p.name IS UNIQUE\",\n",
    "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (t:Topic) REQUIRE t.name IS UNIQUE\"\n",
    "    ]\n",
    "    \n",
    "    for constraint in constraints:\n",
    "        try:\n",
    "            session.run(constraint)\n",
    "            print(f\"Created: {constraint.split('FOR')[1].split('REQUIRE')[0].strip()}\")\n",
    "        except Exception as e:\n",
    "            if \"already exists\" not in str(e):\n",
    "                print(f\"Error creating constraint: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Graph Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_episodes(session, episodes):\n",
    "    \"\"\"Create Episode nodes\"\"\"\n",
    "    query = \"\"\"\n",
    "    UNWIND $episodes AS episode\n",
    "    MERGE (e:Episode {id: episode.id})\n",
    "    SET e.title = episode.title,\n",
    "        e.youtube_video_id = episode.youtube_video_id,\n",
    "        e.youtube_url = episode.youtube_url,\n",
    "        e.duration = episode.duration,\n",
    "        e.published_at = episode.published_at,\n",
    "        e.summary = episode.summary\n",
    "    \"\"\"\n",
    "    session.run(query, episodes=episodes)\n",
    "    print(f\"Created {len(episodes)} Episode nodes\")\n",
    "\n",
    "def populate_segments_and_speakers(session, segments):\n",
    "    \"\"\"Create Segment nodes and Speaker nodes, with relationships\"\"\"\n",
    "    # Create segments\n",
    "    segment_query = \"\"\"\n",
    "    UNWIND $segments AS segment\n",
    "    MERGE (s:Segment {id: segment.id})\n",
    "    SET s.start_time = segment.start_time,\n",
    "        s.end_time = segment.end_time,\n",
    "        s.text = segment.display_text,\n",
    "        s.confidence = segment.confidence,\n",
    "        s.duration = segment.duration,\n",
    "        s.episode_id = segment.episode_id\n",
    "    WITH s, segment\n",
    "    MATCH (e:Episode {id: segment.episode_id})\n",
    "    MERGE (e)-[:HAS_SEGMENT]->(s)\n",
    "    \"\"\"\n",
    "    session.run(segment_query, segments=segments)\n",
    "    print(f\"Created {len(segments)} Segment nodes\")\n",
    "    \n",
    "    # Create speakers and relationships\n",
    "    speaker_query = \"\"\"\n",
    "    UNWIND $segments AS segment\n",
    "    MERGE (sp:Speaker {name: segment.speaker})\n",
    "    WITH sp, segment\n",
    "    MATCH (s:Segment {id: segment.id})\n",
    "    MERGE (sp)-[:SPEAKS_IN]->(s)\n",
    "    \"\"\"\n",
    "    session.run(speaker_query, segments=segments)\n",
    "    \n",
    "    # Create FOLLOWED_BY relationships for temporal flow\n",
    "    temporal_query = \"\"\"\n",
    "    MATCH (s1:Segment), (s2:Segment)\n",
    "    WHERE s1.episode_id = s2.episode_id \n",
    "      AND s1.end_time = s2.start_time\n",
    "    MERGE (s1)-[:FOLLOWED_BY]->(s2)\n",
    "    \"\"\"\n",
    "    session.run(temporal_query)\n",
    "    print(\"Created temporal relationships between segments\")\n",
    "\n",
    "def populate_products(session, products):\n",
    "    \"\"\"Create Product nodes\"\"\"\n",
    "    query = \"\"\"\n",
    "    UNWIND $products AS product\n",
    "    MERGE (p:Product {name: product.name})\n",
    "    SET p.id = product.id,\n",
    "        p.mention_count = product.mention_count\n",
    "    \"\"\"\n",
    "    session.run(query, products=products)\n",
    "    print(f\"Created {len(products)} Product nodes\")\n",
    "\n",
    "def populate_insights(session, insights):\n",
    "    \"\"\"Create Insight nodes and relationships\"\"\"\n",
    "    query = \"\"\"\n",
    "    UNWIND $insights AS insight\n",
    "    MERGE (i:Insight {id: insight.id})\n",
    "    SET i.content = insight.content,\n",
    "        i.category = insight.category,\n",
    "        i.confidence_score = insight.confidence_score,\n",
    "        i.segment_start = insight.segment_start,\n",
    "        i.segment_end = insight.segment_end\n",
    "    WITH i, insight\n",
    "    MATCH (e:Episode {id: insight.episode_id})\n",
    "    MERGE (e)-[:CONTAINS_INSIGHT]->(i)\n",
    "    \"\"\"\n",
    "    session.run(query, insights=insights)\n",
    "    print(f\"Created {len(insights)} Insight nodes\")\n",
    "    \n",
    "    # Create Topic nodes from insight categories\n",
    "    topic_query = \"\"\"\n",
    "    MATCH (i:Insight)\n",
    "    WITH DISTINCT i.category AS category\n",
    "    MERGE (t:Topic {name: category})\n",
    "    SET t.category = 'insight_category'\n",
    "    \"\"\"\n",
    "    session.run(topic_query)\n",
    "    \n",
    "    # Link insights to topics\n",
    "    link_query = \"\"\"\n",
    "    MATCH (i:Insight)\n",
    "    MATCH (t:Topic {name: i.category})\n",
    "    MERGE (i)-[:RELATED_TO]->(t)\n",
    "    \"\"\"\n",
    "    session.run(link_query)\n",
    "    print(\"Created Topic nodes and relationships\")\n",
    "\n",
    "def link_products_to_content(session, segments, products, insights):\n",
    "    \"\"\"Create relationships between products and content\"\"\"\n",
    "    # Get product names for matching\n",
    "    product_names = [p['name'] for p in products]\n",
    "    \n",
    "    # Link segments to products they mention\n",
    "    for segment in segments[:100]:  # Limit for performance\n",
    "        for product_name in product_names:\n",
    "            if product_name.lower() in segment.get('display_text', '').lower():\n",
    "                query = \"\"\"\n",
    "                MATCH (s:Segment {id: $segment_id})\n",
    "                MATCH (p:Product {name: $product_name})\n",
    "                MERGE (s)-[:MENTIONS]->(p)\n",
    "                \"\"\"\n",
    "                session.run(query, segment_id=segment['id'], product_name=product_name)\n",
    "    \n",
    "    # Link insights to products they discuss\n",
    "    for insight in insights:\n",
    "        for product_name in product_names:\n",
    "            if product_name.lower() in insight.get('content', '').lower():\n",
    "                query = \"\"\"\n",
    "                MATCH (i:Insight {id: $insight_id})\n",
    "                MATCH (p:Product {name: $product_name})\n",
    "                MERGE (i)-[:ABOUT]->(p)\n",
    "                \"\"\"\n",
    "                session.run(query, insight_id=insight['id'], product_name=product_name)\n",
    "    \n",
    "    print(\"Created product relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1 Episode nodes\n",
      "Created 90 Segment nodes\n",
      "Created temporal relationships between segments\n",
      "Created 24 Product nodes\n",
      "\n",
      "Graph database population complete!\n"
     ]
    }
   ],
   "source": [
    "# Populate the graph database\n",
    "with driver.session() as session:\n",
    "    if data['episodes']:\n",
    "        populate_episodes(session, data['episodes'])\n",
    "    \n",
    "    if data['segments']:\n",
    "        populate_segments_and_speakers(session, data['segments'])\n",
    "    \n",
    "    if data['products']:\n",
    "        populate_products(session, data['products'])\n",
    "    \n",
    "    if data['insights']:\n",
    "        populate_insights(session, data['insights'])\n",
    "    \n",
    "    if data['segments'] and data['products'] and data['insights']:\n",
    "        link_products_to_content(session, data['segments'], data['products'], data['insights'])\n",
    "\n",
    "print(\"\\nGraph database population complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Graph Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "Episode {id: STRING, title: STRING, youtube_video_id: STRING, youtube_url: STRING, duration: INTEGER, published_at: STRING, summary: STRING}\n",
      "Segment {id: STRING, duration: FLOAT, start_time: FLOAT, end_time: FLOAT, text: STRING, confidence: FLOAT, episode_id: STRING}\n",
      "Speaker {name: STRING}\n",
      "Product {id: STRING, name: STRING, mention_count: INTEGER}\n",
      "Relationship properties:\n",
      "\n",
      "The relationships:\n",
      "(:Episode)-[:HAS_SEGMENT]->(:Segment)\n",
      "(:Segment)-[:FOLLOWED_BY]->(:Segment)\n",
      "(:Speaker)-[:SPEAKS_IN]->(:Segment)\n"
     ]
    }
   ],
   "source": [
    "graph = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USER, password=NEO4J_PW)\n",
    "graph.refresh_schema()\n",
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Enhanced Graph Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "- **Episode**\n",
      "  - `id`: STRING Available options: ['38b926e8-5f64-455e-8045-07436a291f60']\n",
      "  - `title`: STRING Available options: ['The Build - LangChain Open Deep Research']\n",
      "  - `youtube_video_id`: STRING Available options: ['Y0KB8FsbCKc']\n",
      "  - `youtube_url`: STRING Available options: ['https://www.youtube.com/watch?v=Y0KB8FsbCKc&t=525s']\n",
      "  - `duration`: INTEGER Min: 1608, Max: 1608\n",
      "  - `published_at`: STRING Available options: ['2025-07-18T00:00:21.304978']\n",
      "  - `summary`: STRING Available options: ['']\n",
      "- **Segment**\n",
      "  - `id`: STRING Example: \"31a84905-cf4c-4f23-8779-e627d019273a\"\n",
      "  - `duration`: FLOAT Min: 0.36, Max: 90.3\n",
      "  - `start_time`: FLOAT Min: 0.16, Max: 1604.46\n",
      "  - `end_time`: FLOAT Min: 24.32, Max: 1608.06\n",
      "  - `text`: STRING Example: \"I would love for you to talk me through this becau\"\n",
      "  - `confidence`: FLOAT Min: 0.75341797, Max: 0.99922687\n",
      "  - `episode_id`: STRING Available options: ['38b926e8-5f64-455e-8045-07436a291f60']\n",
      "- **Speaker**\n",
      "  - `name`: STRING Available options: ['Speaker A', 'Speaker B']\n",
      "- **Product**\n",
      "  - `id`: STRING Example: \"8acebfc0-611a-4d69-810d-50085c941e98\"\n",
      "  - `name`: STRING Example: \"LangChain\"\n",
      "  - `mention_count`: INTEGER Min: 1, Max: 35\n",
      "Relationship properties:\n",
      "\n",
      "The relationships:\n",
      "(:Episode)-[:HAS_SEGMENT]->(:Segment)\n",
      "(:Segment)-[:FOLLOWED_BY]->(:Segment)\n",
      "(:Speaker)-[:SPEAKS_IN]->(:Segment)\n"
     ]
    }
   ],
   "source": [
    "enhanced_graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USER,\n",
    "    password=NEO4J_PW,\n",
    "    enhanced_schema=True,\n",
    ")\n",
    "print(enhanced_graph.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Queries for YouTube Podcast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which products were discussed most frequently in the podcast? Show top 10 with their mention counts.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Product)\n",
      "RETURN p.name AS product, p.mention_count AS count\n",
      "ORDER BY p.mention_count DESC\n",
      "LIMIT 10;\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'product': 'IO', 'count': 35}, {'product': 'Flow', 'count': 7}, {'product': 'OpenAI', 'count': 5}, {'product': 'Anthropic', 'count': 3}, {'product': 'Claude', 'count': 3}, {'product': 'LangGraph', 'count': 2}, {'product': 'Langgraph', 'count': 2}, {'product': 'ChatGPT', 'count': 2}, {'product': 'LangChain', 'count': 2}, {'product': 'Computer Use', 'count': 1}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Intermediate steps: [{'query': 'MATCH (p:Product)\\nRETURN p.name AS product, p.mention_count AS count\\nORDER BY p.mention_count DESC\\nLIMIT 10;'}, {'context': [{'product': 'IO', 'count': 35}, {'product': 'Flow', 'count': 7}, {'product': 'OpenAI', 'count': 5}, {'product': 'Anthropic', 'count': 3}, {'product': 'Claude', 'count': 3}, {'product': 'LangGraph', 'count': 2}, {'product': 'Langgraph', 'count': 2}, {'product': 'ChatGPT', 'count': 2}, {'product': 'LangChain', 'count': 2}, {'product': 'Computer Use', 'count': 1}]}]\n",
      "Final answer: The top 10 products by mention count are:\n",
      "1. IO – 35 mentions  \n",
      "2. Flow – 7 mentions  \n",
      "3. OpenAI – 5 mentions  \n",
      "4. Anthropic – 3 mentions  \n",
      "5. Claude – 3 mentions  \n",
      "6. LangGraph – 2 mentions  \n",
      "7. Langgraph – 2 mentions  \n",
      "8. ChatGPT – 2 mentions  \n",
      "9. LangChain – 2 mentions  \n",
      "10. Computer Use – 1 mention\n"
     ]
    }
   ],
   "source": [
    "chain = GraphCypherQAChain.from_llm(\n",
    "    ChatOpenAI(model=\"o4-mini\"), \n",
    "    graph=enhanced_graph, \n",
    "    verbose=True, \n",
    "    allow_dangerous_requests=True, \n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "result = chain.invoke({\"query\": query})\n",
    "print(f\"Intermediate steps: {result['intermediate_steps']}\")\n",
    "print(f\"Final answer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GROQ MODELS LIST\n",
    "# --- Production ---\n",
    "# distil-whisper-large-v3-en\n",
    "# gemma2-9b-it\n",
    "# llama-3.1-8b-instant\n",
    "# llama-3.3-70b-versatile\n",
    "# meta-llama/llama-guard-4-12b\n",
    "# whisper-large-v3\n",
    "# whisper-large-v3-turbo\n",
    "\n",
    "# --- Preview ---\n",
    "# deepseek-r1-distill-llama-70b\n",
    "# meta-llama/llama-4-maverick-17b-128e-instruct\n",
    "# meta-llama/llama-4-scout-17b-16e-instruct\n",
    "# meta-llama/llama-prompt-guard-2-22m\n",
    "# meta-llama/llama-prompt-guard-2-86m\n",
    "# mistral-saba-24b\n",
    "# moonshotai/kimi-k2-instruct\n",
    "# playai-tts\n",
    "# playai-tts-arabic\n",
    "# qwen/qwen3-32b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Product)\n",
      "RETURN p.name AS product, p.mention_count AS mentions\n",
      "ORDER BY p.mention_count DESC\n",
      "LIMIT 10\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'product': 'IO', 'mentions': 35}, {'product': 'Flow', 'mentions': 7}, {'product': 'OpenAI', 'mentions': 5}, {'product': 'Anthropic', 'mentions': 3}, {'product': 'Claude', 'mentions': 3}, {'product': 'LangGraph', 'mentions': 2}, {'product': 'Langgraph', 'mentions': 2}, {'product': 'ChatGPT', 'mentions': 2}, {'product': 'LangChain', 'mentions': 2}, {'product': 'Computer Use', 'mentions': 1}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Intermediate steps: [{'query': 'MATCH (p:Product)\\nRETURN p.name AS product, p.mention_count AS mentions\\nORDER BY p.mention_count DESC\\nLIMIT 10'}, {'context': [{'product': 'IO', 'mentions': 35}, {'product': 'Flow', 'mentions': 7}, {'product': 'OpenAI', 'mentions': 5}, {'product': 'Anthropic', 'mentions': 3}, {'product': 'Claude', 'mentions': 3}, {'product': 'LangGraph', 'mentions': 2}, {'product': 'Langgraph', 'mentions': 2}, {'product': 'ChatGPT', 'mentions': 2}, {'product': 'LangChain', 'mentions': 2}, {'product': 'Computer Use', 'mentions': 1}]}]\n",
      "Final answer: The most frequently discussed products in the podcast were:\n",
      "\n",
      "1. IO - 35 mentions\n",
      "2. Flow - 7 mentions\n",
      "3. OpenAI - 5 mentions\n",
      "4. Anthropic - 3 mentions\n",
      "5. Claude - 3 mentions\n",
      "6. LangGraph - 2 mentions\n",
      "7. Langgraph - 2 mentions\n",
      "8. ChatGPT - 2 mentions\n",
      "9. LangChain - 2 mentions\n",
      "10. Computer Use - 1 mention\n"
     ]
    }
   ],
   "source": [
    "model = \"moonshotai/kimi-k2-instruct\"\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    ChatGroq(model=model, temperature=0, max_tokens=None, timeout=None, max_retries=2,), \n",
    "    graph=enhanced_graph, \n",
    "    verbose=True, \n",
    "    allow_dangerous_requests=True, \n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"query\": query})\n",
    "print(f\"Intermediate steps: {result['intermediate_steps']}\")\n",
    "print(f\"Final answer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLLAMA MODELS LIST  \n",
    "# gemma3n:e4b  \n",
    "# deepseek-r1:latest  \n",
    "# devstral:latest  \n",
    "# gemma3:12b  \n",
    "# gemma3:4b  \n",
    "# gemma3:latest  \n",
    "# llama3.2:latest  \n",
    "# llama3.1:latest  \n",
    "# nomic-embed-text:latest  \n",
    "# phi4-mini:latest  \n",
    "# qwen2.5vl:latest  \n",
    "# qwen3:14b  \n",
    "# qwen3:4b  \n",
    "# qwen3:1.7b  \n",
    "# qwen3:0.6b  \n",
    "# qwen3:latest  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3m<think>\n",
      "Okay, let's tackle this problem. The user wants to find the products discussed most frequently in the podcast and show the top 10 with their mention counts. \n",
      "\n",
      "First, I need to look at the schema provided. The relevant nodes here are Product and Segment. The Product node has a mention_count property, but wait, the Segment node has a mention_count? Wait, no. Let me check again. \n",
      "\n",
      "Looking back, the Product node has mention_count as a property. Wait, no. Wait, the Product node's properties are id, name, mention_count. Oh, right, the Product node has mention_count. But how does that relate to the Segments? \n",
      "\n",
      "Wait, the relationships are Episode- HAS_SEGMENT -> Segment, Segment- FOLLOWED_BY -> Segment, and Speaker- SPEAKS_IN -> Segment. There's no direct relationship between Product and Segment. But the question is about products discussed in the podcast. So maybe the Product nodes are connected to Segments somehow? Wait, the schema doesn't mention a relationship between Product and Segment. Hmm, that's a problem. \n",
      "\n",
      "Wait, maybe the mention_count in the Product node is the count of how many times the product was mentioned in the Segments? But how is that connected? Because if there's no relationship between Product and Segment, then how do we aggregate the mentions?\n",
      "\n",
      "Wait, perhaps the mention_count in the Product node is already the total mentions across all Segments. But that doesn't make sense because the Product node's mention_count would need to be updated whenever a Segment mentions it. But the schema doesn't show a relationship between Product and Segment, so maybe that's an error. \n",
      "\n",
      "Alternatively, maybe the mention_count is stored in the Segment node? Wait, no. The Segment node's properties are id, duration, start_time, end_time, text, confidence, episode_id. There's no mention_count there. \n",
      "\n",
      "Wait, the Product node has mention_count. But how do we get that count? Unless there's an implicit relationship that's not listed. But according to the instructions, I can only use the provided relationships. \n",
      "\n",
      "This is confusing. Maybe the mention_count in the Product node is the count of how many times it was mentioned in the Segments. But how to get that? Because there's no direct link between Product and Segment. Unless there's an assumption that each Segment's text contains mentions of Products, and we need to find those. But the schema doesn't have a relationship between Segment and Product. \n",
      "\n",
      "Wait, perhaps the mention_count is stored in the Product node, and we just need to retrieve the top 10 products based on that. But that would mean the mention_count is already aggregated. But the question is about products discussed most frequently, which would require counting how many times each product is mentioned in the Segments. \n",
      "\n",
      "But without a relationship between Product and Segment, how can we do that? Maybe the mention_count in the Product node is the count of mentions. So the answer would be to query the Product nodes and order by mention_count descending, then limit to 10. \n",
      "\n",
      "But that seems too straightforward. Let me check the schema again. The Product node has mention_count as a property. So perhaps the mention_count is already calculated, and the task is to just retrieve the top 10. \n",
      "\n",
      "In that case, the Cypher query would be:\n",
      "\n",
      "MATCH (p:Product)\n",
      "RETURN p.name, p.mention_count\n",
      "ORDER BY p.mention_count DESC\n",
      "LIMIT 10\n",
      "\n",
      "But the problem is, how do we know that the mention_count in the Product node is indeed the count of mentions in the Segments? The schema doesn't specify that. However, given the instructions, maybe that's the intended approach. \n",
      "\n",
      "Alternatively, maybe the mention_count is stored in the Segment node, but the schema says Product has mention_count. \n",
      "\n",
      "So, given the schema as provided, the correct approach is to query the Product nodes, sort by mention_count, and take the top 10. \n",
      "\n",
      "So the final Cypher statement would be:\n",
      "\n",
      "MATCH (p:Product)\n",
      "RETURN p.name, p.mention_count\n",
      "ORDER BY p.mention_count DESC\n",
      "LIMIT 10\n",
      "</think>\n",
      "\n",
      "MATCH (p:Product)\n",
      "RETURN p.name, p.mention_count\n",
      "ORDER BY p.mention_count DESC\n",
      "LIMIT 10\u001b[0m\n"
     ]
    },
    {
     "ename": "CypherSyntaxError",
     "evalue": "{code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input '<': expected 'FOREACH', 'ALTER', 'ORDER BY', 'CALL', 'USING PERIODIC COMMIT', 'CREATE', 'LOAD CSV', 'START DATABASE', 'STOP DATABASE', 'DEALLOCATE', 'DELETE', 'DENY', 'DETACH', 'DROP', 'DRYRUN', 'FINISH', 'GRANT', 'INSERT', 'LIMIT', 'MATCH', 'MERGE', 'NODETACH', 'OFFSET', 'OPTIONAL', 'REALLOCATE', 'REMOVE', 'RENAME', 'RETURN', 'REVOKE', 'ENABLE SERVER', 'SET', 'SHOW', 'SKIP', 'TERMINATE', 'UNWIND', 'USE' or 'WITH' (line 1, column 1 (offset: 0))\n\"<think>\"\n ^}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCypherSyntaxError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m model = \u001b[33m\"\u001b[39m\u001b[33mqwen3:latest\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m chain = GraphCypherQAChain.from_llm(\n\u001b[32m      4\u001b[39m     ChatOllama(base_url=LLM_BASE_URL, model=model), \n\u001b[32m      5\u001b[39m     graph=enhanced_graph, \n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     return_intermediate_steps=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m result = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIntermediate steps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mintermediate_steps\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinal answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages/langchain/chains/base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages/langchain/chains/base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages/langchain_community/chains/graph_qa/cypher.py:392\u001b[39m, in \u001b[36mGraphCypherQAChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;66;03m# Retrieve and limit the number of results\u001b[39;00m\n\u001b[32m    390\u001b[39m \u001b[38;5;66;03m# Generated Cypher be null if query corrector identifies invalid schema\u001b[39;00m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m generated_cypher:\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m     context = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_cypher\u001b[49m\u001b[43m)\u001b[49m[: \u001b[38;5;28mself\u001b[39m.top_k]\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    394\u001b[39m     context = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages/langchain_community/graphs/neo4j_graph.py:467\u001b[39m, in \u001b[36mNeo4jGraph.query\u001b[39m\u001b[34m(self, query, params)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneo4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Neo4jError\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m     data, _, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_driver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mQuery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatabase_\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_database\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters_\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m     json_data = [r.data() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[32m    473\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sanitize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages/neo4j/_sync/driver.py:970\u001b[39m, in \u001b[36mDriver.execute_query\u001b[39m\u001b[34m(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    967\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid routing control value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrouting_\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    968\u001b[39m     )\n\u001b[32m    969\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m session._pipelined_begin:\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_run_transaction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mTelemetryAPI\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDRIVER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_transformer_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages/neo4j/_sync/work/session.py:583\u001b[39m, in \u001b[36mSession._run_transaction\u001b[39m\u001b[34m(self, access_mode, api, transaction_function, args, kwargs)\u001b[39m\n\u001b[32m    581\u001b[39m tx = \u001b[38;5;28mself\u001b[39m._transaction\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m     result = \u001b[43mtransaction_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio.CancelledError:\n\u001b[32m    585\u001b[39m     \u001b[38;5;66;03m# if cancellation callback has not been called yet:\u001b[39;00m\n\u001b[32m    586\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transaction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages/neo4j/_work/query.py:144\u001b[39m, in \u001b[36munit_of_work.<locals>.wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages/neo4j/_sync/driver.py:1306\u001b[39m, in \u001b[36m_work\u001b[39m\u001b[34m(tx, query, parameters, transformer)\u001b[39m\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_work\u001b[39m(\n\u001b[32m   1301\u001b[39m     tx: ManagedTransaction,\n\u001b[32m   1302\u001b[39m     query: te.LiteralString,\n\u001b[32m   1303\u001b[39m     parameters: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, t.Any],\n\u001b[32m   1304\u001b[39m     transformer: t.Callable[[Result], t.Union[_T]],\n\u001b[32m   1305\u001b[39m ) -> _T:\n\u001b[32m-> \u001b[39m\u001b[32m1306\u001b[39m     res = \u001b[43mtx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m transformer(res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages/neo4j/_sync/work/transaction.py:206\u001b[39m, in \u001b[36mTransactionBase.run\u001b[39m\u001b[34m(self, query, parameters, **kwparameters)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28mself\u001b[39m._results.append(result)\n\u001b[32m    205\u001b[39m parameters = \u001b[38;5;28mdict\u001b[39m(parameters \u001b[38;5;129;01mor\u001b[39;00m {}, **kwparameters)\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_tx_ready_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages/neo4j/_sync/work/result.py:177\u001b[39m, in \u001b[36mResult._tx_ready_run\u001b[39m\u001b[34m(self, query, parameters)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_tx_ready_run\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, parameters):\n\u001b[32m    174\u001b[39m     \u001b[38;5;66;03m# BEGIN+RUN does not carry any extra on the RUN message.\u001b[39;00m\n\u001b[32m    175\u001b[39m     \u001b[38;5;66;03m# BEGIN {extra}\u001b[39;00m\n\u001b[32m    176\u001b[39m     \u001b[38;5;66;03m# RUN \"query\" {parameters} {extra}\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages/neo4j/_sync/work/result.py:236\u001b[39m, in \u001b[36mResult._run\u001b[39m\u001b[34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28mself\u001b[39m._pull()\n\u001b[32m    235\u001b[39m \u001b[38;5;28mself\u001b[39m._connection.send_all()\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_attach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages/neo4j/_sync/work/result.py:430\u001b[39m, in \u001b[36mResult._attach\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exhausted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._attached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages/neo4j/_sync/io/_common.py:184\u001b[39m, in \u001b[36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args, **kwargs):\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    186\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio.iscoroutinefunction(\u001b[38;5;28mself\u001b[39m.__on_error)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages/neo4j/_sync/io/_bolt.py:864\u001b[39m, in \u001b[36mBolt.fetch_message\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[32m    861\u001b[39m tag, fields = \u001b[38;5;28mself\u001b[39m.inbox.pop(\n\u001b[32m    862\u001b[39m     hydration_hooks=\u001b[38;5;28mself\u001b[39m.responses[\u001b[32m0\u001b[39m].hydration_hooks\n\u001b[32m    863\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[38;5;28mself\u001b[39m.idle_since = monotonic()\n\u001b[32m    866\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages/neo4j/_sync/io/_bolt5.py:500\u001b[39m, in \u001b[36mBolt5x0._process_message\u001b[39m\u001b[34m(self, tag, fields)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28mself\u001b[39m._server_state_manager.state = \u001b[38;5;28mself\u001b[39m.bolt_states.FAILED\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pool:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.3/libexec/lib/python3.13/site-packages/neo4j/_sync/io/_common.py:254\u001b[39m, in \u001b[36mResponse.on_failure\u001b[39m\u001b[34m(self, metadata)\u001b[39m\n\u001b[32m    252\u001b[39m handler = \u001b[38;5;28mself\u001b[39m.handlers.get(\u001b[33m\"\u001b[39m\u001b[33mon_summary\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    253\u001b[39m Util.callback(handler)\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._hydrate_error(metadata)\n",
      "\u001b[31mCypherSyntaxError\u001b[39m: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input '<': expected 'FOREACH', 'ALTER', 'ORDER BY', 'CALL', 'USING PERIODIC COMMIT', 'CREATE', 'LOAD CSV', 'START DATABASE', 'STOP DATABASE', 'DEALLOCATE', 'DELETE', 'DENY', 'DETACH', 'DROP', 'DRYRUN', 'FINISH', 'GRANT', 'INSERT', 'LIMIT', 'MATCH', 'MERGE', 'NODETACH', 'OFFSET', 'OPTIONAL', 'REALLOCATE', 'REMOVE', 'RENAME', 'RETURN', 'REVOKE', 'ENABLE SERVER', 'SET', 'SHOW', 'SKIP', 'TERMINATE', 'UNWIND', 'USE' or 'WITH' (line 1, column 1 (offset: 0))\n\"<think>\"\n ^}"
     ]
    }
   ],
   "source": [
    "model = \"qwen3:latest\"\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    ChatOllama(base_url=LLM_BASE_URL, model=model), \n",
    "    graph=enhanced_graph, \n",
    "    verbose=True, \n",
    "    allow_dangerous_requests=True, \n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"query\": query})\n",
    "print(f\"Intermediate steps: {result['intermediate_steps']}\")\n",
    "print(f\"Final answer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Example Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query: Timeline of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (e:Episode)-[:HAS_SEGMENT]->(s:Segment)\n",
      "WITH s ORDER BY s.start_time\n",
      "RETURN s.start_time AS start_time, s.text AS insight_category\n",
      "ORDER BY start_time ASC\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'start_time': 0.16, 'insight_category': \"I would love for you to talk me through this because this is pretty relevant. So this is an, an open source project that they've published relatively recently, the LangChain guys. And when they do this they're basically building these agents so that we can, people like us developers can see what can be built. Right. And they're recommended architecture based on their expertise with the platform. Well, the. Sorry, the framework.\"}, {'start_time': 24.88, 'insight_category': \"Yeah, they're really good about when like OpenAI or someone comes out with something that makes a big splash, they turn around and create like an open source version of it.\"}, {'start_time': 34.56, 'insight_category': \"Pretty much the open version, yeah. So whenever there's like they did with computer use, like the demo that I showed you a couple of months, a couple of weeks back. This one is basically a replication of ChatGPT Google Gemini equivalent. This will be interesting because I was literally just chatting to the guys over when I had lunch there with them. I think they're working on an update or an upgrade to this and we were talking through some of these trade offs and how the architectures work to try and trying to like and trying to compare the architecture of say the Gemini deep research approach which apparently is pretty basic and very sequential to the more complicated ones like, like the Claude or the OpenAI ones.\"}, {'start_time': 81.08, 'insight_category': \"I think what's really cool about this for demonstration is that it's not just one pattern necessarily. There are three different designs here. So you can see that I've got it broken up by workflows and then there's a multi agent version. The big difference is the multi agent version allows you to basically converse with an agent about the subject and start like really develop what it is you want to do the research on and then it goes into the multi agent workflow and works with multiple specialized agents like you know, citations agent, stuff like that. So I'll spin this up and show you. It's pretty straightforward to set up to get installed. I don't know if you care to. I already. I guess I can't really do it from scratch because I already set it up.\"}, {'start_time': 138.62, 'insight_category': 'But no, we can share the like the instructions in the link.'}, {'start_time': 143.74, 'insight_category': 'Yeah, if you.'}, {'start_time': 144.54, 'insight_category': \"Yeah, if you just follow those. There's plenty of online, plenty of tutorials online that talk through this. They've probably also got one that goes through the, the process. So yeah, spin it up and show it to us. So just to clarify, did you set it up as the supervisor like that kind of interaction pattern 1 or as the more basic like just do some research, split it out by sections and then combine into the final Version.\"}, {'start_time': 168.4, 'insight_category': \"Well, that's the cool part is you don't have to spin up different. It's all part of the same. Well, they're, they're separate graphs, but it's all the same application. When I run langgraph Dev, we have the ability to switch between all three of them. So got to activate the virtual environment first. I started getting this upgrade error. I guess I thought I did that earlier, but anyways. So Tom, this is what I meant. I just spun up that I just hit Lang Graph dev and now I have the option to select between these three different graphs. So when I select these. That's what's awesome about Langraph. The visual aspect is really helpful. So here we have the ODR workflow and when I was running this with as MCP tool with Claude, this is, this is the right way because this is just a workflow. So Claude kind of does that conversational planning and then kicks it into the workflow once we're ready. So let's do. Yeah, we'll start with the workflow on this one. We will just give it a topic. So let's say Google Gemini cli. So here you can see each step along the way. So it's going to generate the report plan and then will give us a chance to provide input. What was that?\"}, {'start_time': 259.5, 'insight_category': \"So it basically like converts it into some form first. So there's like a chain of like there's a chain behind that. Here's the end. Here are the thinking traces.\"}, {'start_time': 271.69, 'insight_category': 'Yeah, sorry, that jumped.'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Result: Here is the timeline of different insight categories discussed in the episode, ordered by when they first appear:\n",
      "\n",
      "- At 0.16: Discussion about the open source project published by the LangChain team, focusing on building agents and recommended architecture for developers.\n",
      "- At 24.88: Mention of how LangChain quickly creates open source versions of major releases from OpenAI or others.\n",
      "- At 34.56: Explanation that the project replicates ChatGPT and Google Gemini equivalents, including a conversation about architectural trade-offs between different AI models.\n",
      "- At 81.08: Description of three different design patterns in the project, including a multi-agent version that allows conversational interaction and collaboration among specialized agents.\n",
      "- At 138.62: Offer to share setup instructions via a link.\n",
      "- At 144.54: Reference to available online tutorials and a question about which interaction pattern was set up.\n",
      "- At 168.4: Demonstration of switching between three different graphs within the same application and explanation of the visual workflow, including an example using Claude for conversational planning.\n",
      "- At 259.5: Explanation of how the system converts input into a chain with thinking traces.\n"
     ]
    }
   ],
   "source": [
    "timeline_query = \"Show the timeline of different insight categories discussed in the episode, ordered by when they first appear\"\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0), \n",
    "    graph=enhanced_graph, \n",
    "    verbose=True, \n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"query\": timeline_query})\n",
    "print(f\"\\nResult: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query: Speaker Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_query = \"Which speaker spoke the most in the podcast? Show the number of segments for each speaker.\"\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0), \n",
    "    graph=enhanced_graph, \n",
    "    verbose=True, \n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"query\": speaker_query})\n",
    "print(f\"\\nResult: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query: Product Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_insights_query = \"Find all insights that mention MCP or agents. Show the insight content and confidence score.\"\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0), \n",
    "    graph=enhanced_graph, \n",
    "    verbose=True, \n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"query\": product_insights_query})\n",
    "print(f\"\\nResult: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query: Conversation Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_query = \"Show a sample of the conversation flow - find 5 consecutive segments and show who spoke and what they said\"\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0), \n",
    "    graph=enhanced_graph, \n",
    "    verbose=True, \n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"query\": flow_query})\n",
    "print(f\"\\nResult: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Graph Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: MENTIONS)} {position: line: 1, column: 13, offset: 12} for query: 'MATCH ()-[r:MENTIONS]->() RETURN count(r) as count'\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: CONTAINS_INSIGHT)} {position: line: 1, column: 13, offset: 12} for query: 'MATCH ()-[r:CONTAINS_INSIGHT]->() RETURN count(r) as count'\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: ABOUT)} {position: line: 1, column: 13, offset: 12} for query: 'MATCH ()-[r:ABOUT]->() RETURN count(r) as count'\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: RELATED_TO)} {position: line: 1, column: 13, offset: 12} for query: 'MATCH ()-[r:RELATED_TO]->() RETURN count(r) as count'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Statistics:\n",
      "\n",
      "Node Counts:\n",
      "  Episode: 1\n",
      "  Segment: 90\n",
      "  Speaker: 2\n",
      "  Insight: 0\n",
      "  Product: 24\n",
      "  Topic: 0\n",
      "\n",
      "Relationship Counts:\n",
      "  HAS_SEGMENT: 90\n",
      "  SPEAKS_IN: 90\n",
      "  MENTIONS: 0\n",
      "  CONTAINS_INSIGHT: 0\n",
      "  ABOUT: 0\n",
      "  RELATED_TO: 0\n",
      "  FOLLOWED_BY: 26\n",
      "\n",
      "Total Nodes: 117\n",
      "Total Relationships: 206\n"
     ]
    }
   ],
   "source": [
    "# Get node counts\n",
    "with driver.session() as session:\n",
    "    node_counts = {}\n",
    "    for label in ['Episode', 'Segment', 'Speaker', 'Insight', 'Product', 'Topic']:\n",
    "        result = session.run(f\"MATCH (n:{label}) RETURN count(n) as count\")\n",
    "        node_counts[label] = result.single()['count']\n",
    "    \n",
    "    # Get relationship counts\n",
    "    rel_counts = {}\n",
    "    relationships = [\n",
    "        'HAS_SEGMENT', 'SPEAKS_IN', 'MENTIONS', 'CONTAINS_INSIGHT', \n",
    "        'ABOUT', 'RELATED_TO', 'FOLLOWED_BY'\n",
    "    ]\n",
    "    for rel in relationships:\n",
    "        result = session.run(f\"MATCH ()-[r:{rel}]->() RETURN count(r) as count\")\n",
    "        rel_counts[rel] = result.single()['count']\n",
    "\n",
    "print(\"Graph Statistics:\")\n",
    "print(\"\\nNode Counts:\")\n",
    "for label, count in node_counts.items():\n",
    "    print(f\"  {label}: {count}\")\n",
    "\n",
    "print(\"\\nRelationship Counts:\")\n",
    "for rel, count in rel_counts.items():\n",
    "    print(f\"  {rel}: {count}\")\n",
    "\n",
    "print(f\"\\nTotal Nodes: {sum(node_counts.values())}\")\n",
    "print(f\"Total Relationships: {sum(rel_counts.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Product Discussion Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: MENTIONS)} {position: line: 1, column: 23, offset: 22} for query: 'MATCH (p1:Product)<-[:MENTIONS]-(seg:Segment)-[:MENTIONS]->(p2:Product)\\nWHERE p1.id < p2.id\\nRETURN p1.name AS product1, p2.name AS product2, COUNT(DISTINCT seg) AS coMentionCount\\nORDER BY coMentionCount DESC;'\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: MENTIONS)} {position: line: 1, column: 49, offset: 48} for query: 'MATCH (p1:Product)<-[:MENTIONS]-(seg:Segment)-[:MENTIONS]->(p2:Product)\\nWHERE p1.id < p2.id\\nRETURN p1.name AS product1, p2.name AS product2, COUNT(DISTINCT seg) AS coMentionCount\\nORDER BY coMentionCount DESC;'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p1:Product)<-[:MENTIONS]-(seg:Segment)-[:MENTIONS]->(p2:Product)\n",
      "WHERE p1.id < p2.id\n",
      "RETURN p1.name AS product1, p2.name AS product2, COUNT(DISTINCT seg) AS coMentionCount\n",
      "ORDER BY coMentionCount DESC;\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Result: I’m sorry, but I don’t know the answer.\n"
     ]
    }
   ],
   "source": [
    "pattern_query = \"\"\"\n",
    "Which products are most frequently discussed together in the same insights? \n",
    "Show pairs of products that appear in the same insights.\n",
    "\"\"\"\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    ChatOpenAI(model=\"gpt-4.1-mini\"), \n",
    "    graph=enhanced_graph, \n",
    "    verbose=True, \n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"query\": pattern_query})\n",
    "print(f\"\\nResult: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Evolution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (e:Episode)-[:HAS_SEGMENT]->(s:Segment)\n",
      "RETURN s.start_time AS time, s.text AS topic\n",
      "ORDER BY s.start_time ASC\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'time': 0.16, 'topic': \"I would love for you to talk me through this because this is pretty relevant. So this is an, an open source project that they've published relatively recently, the LangChain guys. And when they do this they're basically building these agents so that we can, people like us developers can see what can be built. Right. And they're recommended architecture based on their expertise with the platform. Well, the. Sorry, the framework.\"}, {'time': 24.88, 'topic': \"Yeah, they're really good about when like OpenAI or someone comes out with something that makes a big splash, they turn around and create like an open source version of it.\"}, {'time': 34.56, 'topic': \"Pretty much the open version, yeah. So whenever there's like they did with computer use, like the demo that I showed you a couple of months, a couple of weeks back. This one is basically a replication of ChatGPT Google Gemini equivalent. This will be interesting because I was literally just chatting to the guys over when I had lunch there with them. I think they're working on an update or an upgrade to this and we were talking through some of these trade offs and how the architectures work to try and trying to like and trying to compare the architecture of say the Gemini deep research approach which apparently is pretty basic and very sequential to the more complicated ones like, like the Claude or the OpenAI ones.\"}, {'time': 81.08, 'topic': \"I think what's really cool about this for demonstration is that it's not just one pattern necessarily. There are three different designs here. So you can see that I've got it broken up by workflows and then there's a multi agent version. The big difference is the multi agent version allows you to basically converse with an agent about the subject and start like really develop what it is you want to do the research on and then it goes into the multi agent workflow and works with multiple specialized agents like you know, citations agent, stuff like that. So I'll spin this up and show you. It's pretty straightforward to set up to get installed. I don't know if you care to. I already. I guess I can't really do it from scratch because I already set it up.\"}, {'time': 138.62, 'topic': 'But no, we can share the like the instructions in the link.'}, {'time': 143.74, 'topic': 'Yeah, if you.'}, {'time': 144.54, 'topic': \"Yeah, if you just follow those. There's plenty of online, plenty of tutorials online that talk through this. They've probably also got one that goes through the, the process. So yeah, spin it up and show it to us. So just to clarify, did you set it up as the supervisor like that kind of interaction pattern 1 or as the more basic like just do some research, split it out by sections and then combine into the final Version.\"}, {'time': 168.4, 'topic': \"Well, that's the cool part is you don't have to spin up different. It's all part of the same. Well, they're, they're separate graphs, but it's all the same application. When I run langgraph Dev, we have the ability to switch between all three of them. So got to activate the virtual environment first. I started getting this upgrade error. I guess I thought I did that earlier, but anyways. So Tom, this is what I meant. I just spun up that I just hit Lang Graph dev and now I have the option to select between these three different graphs. So when I select these. That's what's awesome about Langraph. The visual aspect is really helpful. So here we have the ODR workflow and when I was running this with as MCP tool with Claude, this is, this is the right way because this is just a workflow. So Claude kind of does that conversational planning and then kicks it into the workflow once we're ready. So let's do. Yeah, we'll start with the workflow on this one. We will just give it a topic. So let's say Google Gemini cli. So here you can see each step along the way. So it's going to generate the report plan and then will give us a chance to provide input. What was that?\"}, {'time': 259.5, 'topic': \"So it basically like converts it into some form first. So there's like a chain of like there's a chain behind that. Here's the end. Here are the thinking traces.\"}, {'time': 271.69, 'topic': 'Yeah, sorry, that jumped.'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Result: The discussion begins with an introduction to an open source project recently published by the LangChain team, focusing on building agents for developers and showcasing recommended architectures based on their expertise. It then moves on to highlight how LangChain quickly creates open source versions of major new releases from companies like OpenAI, including a replication of ChatGPT and Google Gemini equivalents.\n",
      "\n",
      "As the conversation progresses, there is a comparison of different AI architectures, such as the simpler, sequential approach of Gemini versus the more complex designs of Claude and OpenAI models. The discussion then shifts to exploring three different design patterns within the project, including workflows and a multi-agent version that allows interactive conversations with specialized agents for tasks like citations.\n",
      "\n",
      "Later, the focus turns to practical aspects of setting up and running the application, mentioning the ease of installation, available tutorials, and the ability to switch between different graph designs within the same application. The visual interface of LangGraph is praised for its helpfulness in managing workflows, with an example given of running a workflow using Claude for conversational planning.\n",
      "\n",
      "Towards the end, the conversation delves into the technical details of how the system processes tasks, describing it as converting input into a chain of steps with thinking traces, illustrating the underlying workflow and reasoning process.\n"
     ]
    }
   ],
   "source": [
    "evolution_query = \"\"\"\n",
    "How does the discussion evolve over time? \n",
    "Show the progression of topics (insight categories) throughout the episode timeline.\n",
    "\"\"\"\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0), \n",
    "    graph=enhanced_graph, \n",
    "    verbose=True, \n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"query\": evolution_query})\n",
    "print(f\"\\nResult: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close Neo4j Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()\n",
    "print(\"Neo4j connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLLAMA LLM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
